# 아직 지도학습 부분임

## `Tree-based-ensemble models => Boosting (의사결정나무 발전형)`

- 여러 모델을 섞어서 학습을 한다.
- 대표적으로 XGBoost 와 GradientBoosting이 있다.
- **모델스태킹이 앙상블중에 가장 진보**

### 1. GradientBoosting
- 경사하강법을 사용해 성능이 좋다. 다만 학습시간이 오래걸리는 단점이 있다. 단점을 커버하는 발전형으로 light GBM 이 있다.

### 2. XGBoost(Extreme GradientBoosting)
- 병렬 처리 기법을 적용하여 위 1. 보다 학습속도가 빠른 장점이 있다. 그러나 HPO가 40개 이상이라는 단점이 있다. HyperParameterOptimization을 일일이 적용해서 최적의 하이퍼 파라미터를 찾아내는게 불가능에 가깝다.

## `분석 평가`
- Numerical 자료형엔 r2 score
- Classification 자료형엔 accuracy

## `Feature importance(X값 데이터 열들 중 중요한 열을 순서대로 찾아내는 모델)`
- permutation_importance => X값 중요도 파악에 좋은 모델
- 현재 머신러닝은 Feature importance로 Explainable AI(설명 가능한 인공지능 => XAI)가 가능하나 딥러닝은 불투명한 상태이다.

<br>
<br>
<br>
<br>

# 비지도학습

## `군집화(Clustering)`

K-means algorithm (군집화 알고리즘)

- 1. 임의의 중심값 생성
- 2. 각 중심값과 가까운 X들을 해당 중심값의 class로 분류
- 3. 그렇게 생성된 군집의 평균지점을 다시 중심값으로
- 4. K값(HPO) 정해준 수치만큼 반복

<br>
<br>

## `군집 평가`

> 좋은 cluster 조건
> 1. 자기 클러스터끼리 최대한 가까이 뭉친다.
> 2. 가장 가까운 다른 클러스터와 최대한 멀어야 한다.

실루엣 분석
- 위 좋은 cluster조건을 보는 분석방법이다.
- 동일 군집끼리 얼마나 뭉쳤고, 다른 군집과는 얼마나 떨어졌는지 실루엣계수로 파악을 할 수 있게 해준다.

## `차원 축소`
- PCA(Principal Component Analysis) => 주성분 분석. 차원 축소의 한 방법

- 기본적으로 사용시 성능이 떨어지기 때문에 해야할 이유가 없다면 하지 않는다. 해야하는 이유는 너무 많은 열들로 인하여 과부하가 걸릴 때.

- 사용된 데이터가 95% 이상이면 OK => 손실된 데이터가 5%미만일때 OK



<br>
<br>
<br>
<br>


# 오늘의 용어
1. deviance => '편차' 라는 뜻도 있지만, '오차'라는 뜻도 있다. == loss (.loss 등 뭔가 사용할 일이 있다면 numerical 자료형에만 사용가능하다. 만약 classification 자료형을 .loss등 오차 편차 이름의 명령어를 대입하면 당연히 오류가 뜬다.)

2. inertia(관성)

- 군집한 클러스터들이 각각 센터까지 거리 제곱의 합

# sklearn process
- part 1 => 108page 참고