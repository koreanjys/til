{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e99207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80c1e2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samsung크롤링 시작\n",
      "1페이지완료  2페이지완료  3페이지완료  4페이지완료  5페이지완료  6페이지완료  7페이지완료  8페이지완료  9페이지완료  10페이지완료  11페이지완료  12페이지완료  13페이지완료  14페이지완료  15페이지완료  16페이지완료  17페이지완료  18페이지완료  19페이지완료  20페이지완료  21페이지완료  22페이지완료  23페이지완료  24페이지완료  25페이지완료  26페이지완료  27페이지완료  28페이지완료  29페이지완료  30페이지완료  samsung크롤링 완료\n",
      "skhynix크롤링 시작\n",
      "1페이지완료  2페이지완료  3페이지완료  4페이지완료  5페이지완료  6페이지완료  7페이지완료  8페이지완료  9페이지완료  10페이지완료  11페이지완료  12페이지완료  13페이지완료  14페이지완료  15페이지완료  16페이지완료  17페이지완료  18페이지완료  19페이지완료  20페이지완료  21페이지완료  22페이지완료  23페이지완료  24페이지완료  25페이지완료  26페이지완료  27페이지완료  28페이지완료  29페이지완료  30페이지완료  skhynix크롤링 완료\n",
      "sksquare크롤링 시작\n",
      "1페이지완료  2페이지완료  3페이지완료  4페이지완료  5페이지완료  6페이지완료  7페이지완료  8페이지완료  9페이지완료  10페이지완료  11페이지완료  12페이지완료  13페이지완료  14페이지완료  15페이지완료  16페이지완료  17페이지완료  18페이지완료  19페이지완료  20페이지완료  21페이지완료  22페이지완료  23페이지완료  24페이지완료  25페이지완료  26페이지완료  27페이지완료  28페이지완료  29페이지완료  30페이지완료  sksquare크롤링 완료\n",
      "dbhitek크롤링 시작\n",
      "1페이지완료  2페이지완료  3페이지완료  4페이지완료  5페이지완료  6페이지완료  7페이지완료  8페이지완료  9페이지완료  10페이지완료  11페이지완료  12페이지완료  13페이지완료  14페이지완료  15페이지완료  16페이지완료  17페이지완료  18페이지완료  19페이지완료  20페이지완료  21페이지완료  22페이지완료  23페이지완료  24페이지완료  25페이지완료  26페이지완료  27페이지완료  28페이지완료  29페이지완료  30페이지완료  dbhitek크롤링 완료\n"
     ]
    }
   ],
   "source": [
    "# 크롬으로 웹창 시작  # 한번에 4개 csv 파일 만들 수 있도록 정리 요망\n",
    "browser = webdriver.Chrome()\n",
    "# 증권 데이터 url 정의  (삼성=005930, SK하이닉스=000660, SK스퀘어=402340, DB하이텍=000990)\n",
    "datas = [['samsung', '005930'],['skhynix', '000660'], ['sksquare', '402340'], ['dbhitek', '000990']]\n",
    "\n",
    "for com_name, com_code in datas:\n",
    "    page = 1\n",
    "    print(f'{com_name}크롤링 시작\\n')\n",
    "    for p in range(1, 31):\n",
    "        url = f'https://finance.naver.com/item/sise_day.naver?code={com_code}&page={page}'\n",
    "        # url 정보 가져오기\n",
    "        browser.get(url)\n",
    "        # 삼성 데이터 추출\n",
    "        com_df = pd.read_html(browser.page_source)[0]\n",
    "        # 모든 행의 요소가 결측치일 경우 행 삭제\n",
    "        com_df.dropna(axis='index', how='all', inplace=True)\n",
    "        # 전일비 culumn 삭제\n",
    "        com_df.drop(['전일비'], axis=1, inplace=True)\n",
    "        # csv파일로 저장\n",
    "        com_info = f'{com_name}.csv'\n",
    "        if os.path.exists(com_info):\n",
    "            com_df.to_csv(com_info, encoding='utf-8-sig', index=False, mode='a', header=False)\n",
    "        else:\n",
    "            com_df.to_csv(com_info, encoding='utf-8-sig', index=False)\n",
    "        print(f'{page}페이지완료', end='  ')\n",
    "        page += 1\n",
    "    print(f'\\n{com_name}크롤링 완료')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
